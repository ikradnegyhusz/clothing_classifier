{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import preprocessing as pre\n",
    "from forward_feed_nn import NeuralNetwork\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.load('../data/fashion_train.npy')\n",
    "\n",
    "df = pre.preprocess(df)\n",
    "\n",
    "K_classes = np.unique(df[:, -1])\n",
    "\n",
    "X_train = df[:,:-1]\n",
    "y_train = df[:,-1]\n",
    "\n",
    "#load test data\n",
    "df = np.load('../data/fashion_test.npy')\n",
    "\n",
    "df = pre.preprocess(df)\n",
    "\n",
    "X_test = df[:,:-1]\n",
    "y_test = df[:,-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_input_layer = int(X_train.shape[1])\n",
    "\n",
    "X_test_inputlayer = int(X_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.6209\n",
      "Epoch 10, Loss: 0.4483\n",
      "Epoch 20, Loss: 0.8073\n",
      "Epoch 30, Loss: 0.4082\n",
      "Epoch 40, Loss: 0.8283\n",
      "Epoch 50, Loss: 0.2101\n",
      "Epoch 60, Loss: 0.5551\n",
      "Epoch 70, Loss: 0.6158\n",
      "Epoch 80, Loss: 0.4472\n",
      "Epoch 90, Loss: 0.4892\n",
      "Layers: [65, 128, 5], Dropout: 0, Accuracy: 0.8238, Learning Rate: 0.001\n",
      "Epoch 0, Loss: 0.2254\n",
      "Epoch 10, Loss: 0.1880\n",
      "Epoch 20, Loss: 0.5282\n",
      "Epoch 30, Loss: 0.2297\n",
      "Epoch 40, Loss: 0.0736\n",
      "Epoch 50, Loss: 0.2859\n",
      "Epoch 60, Loss: 0.3931\n",
      "Epoch 70, Loss: 0.0991\n",
      "Epoch 80, Loss: 0.0855\n",
      "Epoch 90, Loss: 0.6889\n",
      "Layers: [65, 128, 5], Dropout: 0, Accuracy: 0.8458, Learning Rate: 0.01\n",
      "Epoch 0, Loss: 0.2239\n",
      "Epoch 10, Loss: 0.2664\n",
      "Epoch 20, Loss: 0.0562\n",
      "Epoch 30, Loss: 0.0411\n",
      "Epoch 40, Loss: 0.0397\n",
      "Epoch 50, Loss: 0.1667\n",
      "Epoch 60, Loss: 0.0145\n",
      "Epoch 70, Loss: 0.0096\n",
      "Epoch 80, Loss: 0.0016\n",
      "Epoch 90, Loss: 0.0117\n",
      "Layers: [65, 128, 5], Dropout: 0, Accuracy: 0.848, Learning Rate: 0.1\n",
      "Epoch 0, Loss: 1.1569\n",
      "Epoch 10, Loss: 0.2534\n",
      "Epoch 20, Loss: 0.4727\n",
      "Epoch 30, Loss: 0.3936\n",
      "Epoch 40, Loss: 0.4557\n",
      "Epoch 50, Loss: 0.4961\n",
      "Epoch 60, Loss: 0.2966\n",
      "Epoch 70, Loss: 0.6737\n",
      "Epoch 80, Loss: 0.2400\n",
      "Epoch 90, Loss: 0.9261\n",
      "Layers: [65, 128, 5], Dropout: 0.2, Accuracy: 0.8256, Learning Rate: 0.001\n",
      "Epoch 0, Loss: 1.2399\n",
      "Epoch 10, Loss: 0.3631\n",
      "Epoch 20, Loss: 0.3632\n",
      "Epoch 30, Loss: 0.3022\n",
      "Epoch 40, Loss: 0.6051\n",
      "Epoch 50, Loss: 0.7860\n",
      "Epoch 60, Loss: 0.2672\n",
      "Epoch 70, Loss: 0.3348\n",
      "Epoch 80, Loss: 0.3426\n",
      "Epoch 90, Loss: 0.7751\n",
      "Layers: [65, 128, 5], Dropout: 0.2, Accuracy: 0.8504, Learning Rate: 0.01\n",
      "Epoch 0, Loss: 0.6694\n",
      "Epoch 10, Loss: 0.7199\n",
      "Epoch 20, Loss: 0.1348\n",
      "Epoch 30, Loss: 0.1317\n",
      "Epoch 40, Loss: 0.0488\n",
      "Epoch 50, Loss: 0.0535\n",
      "Epoch 60, Loss: 0.3157\n",
      "Epoch 70, Loss: 0.1401\n",
      "Epoch 80, Loss: 0.0953\n",
      "Epoch 90, Loss: 0.1718\n",
      "Layers: [65, 128, 5], Dropout: 0.2, Accuracy: 0.8614, Learning Rate: 0.1\n",
      "Epoch 0, Loss: 1.7243\n",
      "Epoch 10, Loss: 0.7848\n",
      "Epoch 20, Loss: 0.6022\n",
      "Epoch 30, Loss: 0.5466\n",
      "Epoch 40, Loss: 0.3118\n",
      "Epoch 50, Loss: 0.4504\n",
      "Epoch 60, Loss: 0.3850\n",
      "Epoch 70, Loss: 0.2664\n",
      "Epoch 80, Loss: 0.6732\n",
      "Epoch 90, Loss: 0.2432\n",
      "Layers: [65, 128, 5], Dropout: 0.3, Accuracy: 0.8242, Learning Rate: 0.001\n",
      "Epoch 0, Loss: 0.8114\n",
      "Epoch 10, Loss: 0.4492\n",
      "Epoch 20, Loss: 0.5579\n",
      "Epoch 30, Loss: 0.4507\n",
      "Epoch 40, Loss: 0.2630\n",
      "Epoch 50, Loss: 0.3031\n",
      "Epoch 60, Loss: 0.3936\n",
      "Epoch 70, Loss: 0.8323\n",
      "Epoch 80, Loss: 0.6167\n",
      "Epoch 90, Loss: 0.3752\n",
      "Layers: [65, 128, 5], Dropout: 0.3, Accuracy: 0.846, Learning Rate: 0.01\n",
      "Epoch 0, Loss: 0.4211\n",
      "Epoch 10, Loss: 0.4135\n",
      "Epoch 20, Loss: 0.3171\n",
      "Epoch 30, Loss: 0.0501\n",
      "Epoch 40, Loss: 0.3032\n",
      "Epoch 50, Loss: 0.2365\n",
      "Epoch 60, Loss: 0.5909\n",
      "Epoch 70, Loss: 0.4279\n",
      "Epoch 80, Loss: 0.1300\n",
      "Epoch 90, Loss: 0.0968\n",
      "Layers: [65, 128, 5], Dropout: 0.3, Accuracy: 0.8532, Learning Rate: 0.1\n",
      "Epoch 0, Loss: 1.8969\n",
      "Epoch 10, Loss: 0.8939\n",
      "Epoch 20, Loss: 0.5496\n",
      "Epoch 30, Loss: 0.6712\n",
      "Epoch 40, Loss: 0.1209\n",
      "Epoch 50, Loss: 0.1954\n",
      "Epoch 60, Loss: 0.6195\n",
      "Epoch 70, Loss: 0.4946\n",
      "Epoch 80, Loss: 0.4404\n",
      "Epoch 90, Loss: 0.3391\n",
      "Layers: [65, 128, 5], Dropout: 0.5, Accuracy: 0.821, Learning Rate: 0.001\n",
      "Epoch 0, Loss: 1.6470\n",
      "Epoch 10, Loss: 0.5090\n",
      "Epoch 20, Loss: 0.3801\n",
      "Epoch 30, Loss: 0.3625\n",
      "Epoch 40, Loss: 0.6693\n",
      "Epoch 50, Loss: 0.5531\n",
      "Epoch 60, Loss: 0.4717\n",
      "Epoch 70, Loss: 0.5916\n",
      "Epoch 80, Loss: 0.2407\n",
      "Epoch 90, Loss: 0.3460\n",
      "Layers: [65, 128, 5], Dropout: 0.5, Accuracy: 0.8372, Learning Rate: 0.01\n",
      "Epoch 0, Loss: 0.6703\n",
      "Epoch 10, Loss: 0.4859\n",
      "Epoch 20, Loss: 0.5071\n",
      "Epoch 30, Loss: 0.2881\n",
      "Epoch 40, Loss: 1.0284\n",
      "Epoch 50, Loss: 0.1239\n",
      "Epoch 60, Loss: 0.4824\n",
      "Epoch 70, Loss: 0.1675\n",
      "Epoch 80, Loss: 0.1496\n",
      "Epoch 90, Loss: 0.3442\n",
      "Layers: [65, 128, 5], Dropout: 0.5, Accuracy: 0.859, Learning Rate: 0.1\n",
      "Epoch 0, Loss: 0.6853\n",
      "Epoch 10, Loss: 0.3218\n",
      "Epoch 20, Loss: 0.5896\n",
      "Epoch 30, Loss: 0.7985\n",
      "Epoch 40, Loss: 0.4536\n",
      "Epoch 50, Loss: 0.4857\n",
      "Epoch 60, Loss: 0.4896\n",
      "Epoch 70, Loss: 0.2272\n",
      "Epoch 80, Loss: 0.1399\n",
      "Epoch 90, Loss: 0.6257\n",
      "Layers: [65, 256, 128, 5], Dropout: 0, Accuracy: 0.8416, Learning Rate: 0.001\n",
      "Epoch 0, Loss: 0.2404\n",
      "Epoch 10, Loss: 0.1930\n",
      "Epoch 20, Loss: 0.0890\n",
      "Epoch 30, Loss: 0.1234\n",
      "Epoch 40, Loss: 0.1124\n",
      "Epoch 50, Loss: 0.1274\n",
      "Epoch 60, Loss: 0.0378\n",
      "Epoch 70, Loss: 0.0785\n",
      "Epoch 80, Loss: 0.0194\n",
      "Epoch 90, Loss: 0.0681\n",
      "Layers: [65, 256, 128, 5], Dropout: 0, Accuracy: 0.8458, Learning Rate: 0.01\n",
      "Epoch 0, Loss: 0.1383\n",
      "Epoch 10, Loss: 0.0570\n",
      "Epoch 20, Loss: 0.0331\n",
      "Epoch 30, Loss: 0.0021\n",
      "Epoch 40, Loss: 0.0053\n",
      "Epoch 50, Loss: 0.0006\n",
      "Epoch 60, Loss: 0.0018\n",
      "Epoch 70, Loss: 0.0020\n",
      "Epoch 80, Loss: 0.0017\n",
      "Epoch 90, Loss: 0.0022\n",
      "Layers: [65, 256, 128, 5], Dropout: 0, Accuracy: 0.8544, Learning Rate: 0.1\n",
      "Epoch 0, Loss: 2.3075\n",
      "Epoch 10, Loss: 0.7384\n",
      "Epoch 20, Loss: 0.4568\n",
      "Epoch 30, Loss: 0.2402\n",
      "Epoch 40, Loss: 0.6246\n",
      "Epoch 50, Loss: 0.3275\n",
      "Epoch 60, Loss: 0.3642\n",
      "Epoch 70, Loss: 0.7202\n",
      "Epoch 80, Loss: 0.7147\n",
      "Epoch 90, Loss: 0.3097\n",
      "Layers: [65, 256, 128, 5], Dropout: 0.2, Accuracy: 0.831, Learning Rate: 0.001\n",
      "Epoch 0, Loss: 0.7606\n",
      "Epoch 10, Loss: 0.1144\n",
      "Epoch 20, Loss: 0.2303\n",
      "Epoch 30, Loss: 0.2953\n",
      "Epoch 40, Loss: 0.3129\n",
      "Epoch 50, Loss: 0.1394\n",
      "Epoch 60, Loss: 0.2346\n",
      "Epoch 70, Loss: 0.2514\n",
      "Epoch 80, Loss: 0.0284\n",
      "Epoch 90, Loss: 0.3118\n",
      "Layers: [65, 256, 128, 5], Dropout: 0.2, Accuracy: 0.858, Learning Rate: 0.01\n",
      "Epoch 0, Loss: 0.4546\n",
      "Epoch 10, Loss: 0.3770\n",
      "Epoch 20, Loss: 0.3153\n",
      "Epoch 30, Loss: 0.1903\n",
      "Epoch 40, Loss: 0.3643\n",
      "Epoch 50, Loss: 0.0335\n",
      "Epoch 60, Loss: 0.0232\n",
      "Epoch 70, Loss: 0.3033\n",
      "Epoch 80, Loss: 0.0297\n",
      "Epoch 90, Loss: 0.1361\n",
      "Layers: [65, 256, 128, 5], Dropout: 0.2, Accuracy: 0.8632, Learning Rate: 0.1\n",
      "Epoch 0, Loss: 4.2990\n",
      "Epoch 10, Loss: 0.5468\n",
      "Epoch 20, Loss: 0.7310\n",
      "Epoch 30, Loss: 0.4967\n",
      "Epoch 40, Loss: 0.6860\n",
      "Epoch 50, Loss: 0.6754\n",
      "Epoch 60, Loss: 0.9213\n",
      "Epoch 70, Loss: 0.7661\n",
      "Epoch 80, Loss: 0.4635\n",
      "Epoch 90, Loss: 0.5968\n",
      "Layers: [65, 256, 128, 5], Dropout: 0.3, Accuracy: 0.8138, Learning Rate: 0.001\n",
      "Epoch 0, Loss: 0.9216\n",
      "Epoch 10, Loss: 0.2833\n",
      "Epoch 20, Loss: 0.3930\n",
      "Epoch 30, Loss: 0.4028\n",
      "Epoch 40, Loss: 0.3138\n",
      "Epoch 50, Loss: 0.8033\n",
      "Epoch 60, Loss: 0.3211\n",
      "Epoch 70, Loss: 0.4663\n",
      "Epoch 80, Loss: 0.1868\n",
      "Epoch 90, Loss: 0.1872\n",
      "Layers: [65, 256, 128, 5], Dropout: 0.3, Accuracy: 0.8456, Learning Rate: 0.01\n",
      "Epoch 0, Loss: 0.3802\n",
      "Epoch 10, Loss: 0.1599\n",
      "Epoch 20, Loss: 0.5812\n",
      "Epoch 30, Loss: 0.5503\n",
      "Epoch 40, Loss: 0.1299\n",
      "Epoch 50, Loss: 0.2487\n",
      "Epoch 60, Loss: 0.8600\n",
      "Epoch 70, Loss: 0.2031\n",
      "Epoch 80, Loss: 0.6404\n",
      "Epoch 90, Loss: 0.3515\n",
      "Layers: [65, 256, 128, 5], Dropout: 0.3, Accuracy: 0.8586, Learning Rate: 0.1\n",
      "Epoch 0, Loss: 3.3990\n",
      "Epoch 10, Loss: 1.4739\n",
      "Epoch 20, Loss: 1.0227\n",
      "Epoch 30, Loss: 0.8464\n",
      "Epoch 40, Loss: 0.9085\n",
      "Epoch 50, Loss: 0.8776\n",
      "Epoch 60, Loss: 1.1175\n",
      "Epoch 70, Loss: 0.7002\n",
      "Epoch 80, Loss: 0.7152\n",
      "Epoch 90, Loss: 0.5885\n",
      "Layers: [65, 256, 128, 5], Dropout: 0.5, Accuracy: 0.7918, Learning Rate: 0.001\n",
      "Epoch 0, Loss: 0.9875\n",
      "Epoch 10, Loss: 1.0523\n",
      "Epoch 20, Loss: 0.4560\n",
      "Epoch 30, Loss: 0.6373\n",
      "Epoch 40, Loss: 0.3733\n",
      "Epoch 50, Loss: 0.6245\n",
      "Epoch 60, Loss: 0.3792\n",
      "Epoch 70, Loss: 0.3774\n",
      "Epoch 80, Loss: 0.2533\n",
      "Epoch 90, Loss: 0.9882\n",
      "Layers: [65, 256, 128, 5], Dropout: 0.5, Accuracy: 0.824, Learning Rate: 0.01\n",
      "Epoch 0, Loss: 0.7025\n",
      "Epoch 10, Loss: 0.4421\n",
      "Epoch 20, Loss: 0.3608\n",
      "Epoch 30, Loss: 0.2056\n",
      "Epoch 40, Loss: 0.4430\n",
      "Epoch 50, Loss: 0.1134\n",
      "Epoch 60, Loss: 0.1488\n",
      "Epoch 70, Loss: 0.2805\n",
      "Epoch 80, Loss: 0.3078\n",
      "Epoch 90, Loss: 0.2561\n",
      "Layers: [65, 256, 128, 5], Dropout: 0.5, Accuracy: 0.852, Learning Rate: 0.1\n",
      "Epoch 0, Loss: 0.4413\n",
      "Epoch 10, Loss: 0.9858\n",
      "Epoch 20, Loss: 0.7096\n",
      "Epoch 30, Loss: 0.4566\n",
      "Epoch 40, Loss: 0.2007\n",
      "Epoch 50, Loss: 0.1641\n",
      "Epoch 60, Loss: 0.1474\n",
      "Epoch 70, Loss: 0.2347\n",
      "Epoch 80, Loss: 0.3825\n",
      "Epoch 90, Loss: 0.1930\n",
      "Layers: [65, 512, 256, 128, 5], Dropout: 0, Accuracy: 0.844, Learning Rate: 0.001\n",
      "Epoch 0, Loss: 0.2435\n",
      "Epoch 10, Loss: 0.2830\n",
      "Epoch 20, Loss: 0.1352\n",
      "Epoch 30, Loss: 0.0432\n",
      "Epoch 40, Loss: 0.0241\n",
      "Epoch 50, Loss: 0.0622\n",
      "Epoch 60, Loss: 0.0212\n",
      "Epoch 70, Loss: 0.0047\n",
      "Epoch 80, Loss: 0.0059\n",
      "Epoch 90, Loss: 0.0078\n",
      "Layers: [65, 512, 256, 128, 5], Dropout: 0, Accuracy: 0.8508, Learning Rate: 0.01\n",
      "Epoch 0, Loss: 0.6103\n",
      "Epoch 10, Loss: 0.2303\n",
      "Epoch 20, Loss: 0.0344\n",
      "Epoch 30, Loss: 0.0014\n",
      "Epoch 40, Loss: 0.0014\n",
      "Epoch 50, Loss: 0.0002\n",
      "Epoch 60, Loss: 0.0003\n",
      "Epoch 70, Loss: 0.0004\n",
      "Epoch 80, Loss: 0.0001\n",
      "Epoch 90, Loss: 0.0001\n",
      "Layers: [65, 512, 256, 128, 5], Dropout: 0, Accuracy: 0.858, Learning Rate: 0.1\n",
      "Epoch 0, Loss: 2.4187\n",
      "Epoch 10, Loss: 0.9360\n",
      "Epoch 20, Loss: 0.3045\n",
      "Epoch 30, Loss: 0.3676\n",
      "Epoch 40, Loss: 0.5834\n",
      "Epoch 50, Loss: 0.4917\n",
      "Epoch 60, Loss: 0.2583\n",
      "Epoch 70, Loss: 0.6869\n",
      "Epoch 80, Loss: 0.7143\n",
      "Epoch 90, Loss: 0.3952\n",
      "Layers: [65, 512, 256, 128, 5], Dropout: 0.2, Accuracy: 0.8372, Learning Rate: 0.001\n",
      "Epoch 0, Loss: 0.5014\n",
      "Epoch 10, Loss: 0.1170\n",
      "Epoch 20, Loss: 0.4698\n",
      "Epoch 30, Loss: 0.4000\n",
      "Epoch 40, Loss: 0.3107\n",
      "Epoch 50, Loss: 0.4543\n",
      "Epoch 60, Loss: 0.4237\n",
      "Epoch 70, Loss: 0.2953\n",
      "Epoch 80, Loss: 0.1259\n",
      "Epoch 90, Loss: 0.3396\n",
      "Layers: [65, 512, 256, 128, 5], Dropout: 0.2, Accuracy: 0.8558, Learning Rate: 0.01\n",
      "Epoch 0, Loss: 0.1720\n",
      "Epoch 10, Loss: 0.1158\n",
      "Epoch 20, Loss: 0.4198\n",
      "Epoch 30, Loss: 0.0461\n",
      "Epoch 40, Loss: 0.1275\n",
      "Epoch 50, Loss: 0.2802\n",
      "Epoch 60, Loss: 0.0469\n",
      "Epoch 70, Loss: 0.1019\n",
      "Epoch 80, Loss: 0.0093\n",
      "Epoch 90, Loss: 0.0352\n",
      "Layers: [65, 512, 256, 128, 5], Dropout: 0.2, Accuracy: 0.8648, Learning Rate: 0.1\n",
      "Epoch 0, Loss: 2.3316\n",
      "Epoch 10, Loss: 0.9415\n",
      "Epoch 20, Loss: 0.8181\n",
      "Epoch 30, Loss: 0.6761\n",
      "Epoch 40, Loss: 1.0972\n",
      "Epoch 50, Loss: 0.8383\n",
      "Epoch 60, Loss: 0.5168\n",
      "Epoch 70, Loss: 0.5735\n",
      "Epoch 80, Loss: 0.7515\n",
      "Epoch 90, Loss: 0.2885\n",
      "Layers: [65, 512, 256, 128, 5], Dropout: 0.3, Accuracy: 0.8148, Learning Rate: 0.001\n",
      "Epoch 0, Loss: 0.5779\n",
      "Epoch 10, Loss: 0.5195\n",
      "Epoch 20, Loss: 0.3625\n",
      "Epoch 30, Loss: 0.2292\n",
      "Epoch 40, Loss: 0.5744\n",
      "Epoch 50, Loss: 0.3695\n",
      "Epoch 60, Loss: 0.3199\n",
      "Epoch 70, Loss: 0.5473\n",
      "Epoch 80, Loss: 0.3607\n",
      "Epoch 90, Loss: 0.3754\n",
      "Layers: [65, 512, 256, 128, 5], Dropout: 0.3, Accuracy: 0.8436, Learning Rate: 0.01\n",
      "Epoch 0, Loss: 0.8766\n",
      "Epoch 10, Loss: 0.4869\n",
      "Epoch 20, Loss: 0.4261\n",
      "Epoch 30, Loss: 0.7198\n",
      "Epoch 40, Loss: 0.3082\n",
      "Epoch 50, Loss: 0.3290\n",
      "Epoch 60, Loss: 0.3151\n",
      "Epoch 70, Loss: 0.1219\n",
      "Epoch 80, Loss: 0.2320\n",
      "Epoch 90, Loss: 0.0633\n",
      "Layers: [65, 512, 256, 128, 5], Dropout: 0.3, Accuracy: 0.8636, Learning Rate: 0.1\n",
      "Epoch 0, Loss: 3.9151\n",
      "Epoch 10, Loss: 1.0462\n",
      "Epoch 20, Loss: 0.9005\n",
      "Epoch 30, Loss: 1.1805\n",
      "Epoch 40, Loss: 0.7206\n",
      "Epoch 50, Loss: 0.8784\n",
      "Epoch 60, Loss: 0.9812\n",
      "Epoch 70, Loss: 0.4344\n",
      "Epoch 80, Loss: 0.9450\n",
      "Epoch 90, Loss: 0.7335\n",
      "Layers: [65, 512, 256, 128, 5], Dropout: 0.5, Accuracy: 0.7368, Learning Rate: 0.001\n",
      "Epoch 0, Loss: 1.4123\n",
      "Epoch 10, Loss: 0.5793\n",
      "Epoch 20, Loss: 0.7419\n",
      "Epoch 30, Loss: 0.9503\n",
      "Epoch 40, Loss: 0.7889\n",
      "Epoch 50, Loss: 0.7028\n",
      "Epoch 60, Loss: 0.3383\n",
      "Epoch 70, Loss: 0.5358\n",
      "Epoch 80, Loss: 0.3533\n",
      "Epoch 90, Loss: 0.8947\n",
      "Layers: [65, 512, 256, 128, 5], Dropout: 0.5, Accuracy: 0.8166, Learning Rate: 0.01\n",
      "Epoch 0, Loss: 1.1769\n",
      "Epoch 10, Loss: 0.4506\n",
      "Epoch 20, Loss: 0.3177\n",
      "Epoch 30, Loss: 0.3267\n",
      "Epoch 40, Loss: 1.0151\n",
      "Epoch 50, Loss: 0.5009\n",
      "Epoch 60, Loss: 0.3621\n",
      "Epoch 70, Loss: 0.2978\n",
      "Epoch 80, Loss: 0.5258\n",
      "Epoch 90, Loss: 0.4402\n",
      "Layers: [65, 512, 256, 128, 5], Dropout: 0.5, Accuracy: 0.8316, Learning Rate: 0.1\n"
     ]
    }
   ],
   "source": [
    "layer_options = [[X_train_input_layer, 128, 5], [X_train_input_layer, 256, 128, 5], [X_train_input_layer, 512, 256, 128, 5]]\n",
    "dropout_rates = [0, 0.2, 0.3, 0.5]\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "\n",
    "best_HP = {\n",
    "\n",
    "}\n",
    "\n",
    "for layers in layer_options:\n",
    "    for dropout in dropout_rates:\n",
    "        for learning in learning_rates:\n",
    "            nn = NeuralNetwork(layers, learning_rate=learning, dropout_rate=dropout)\n",
    "            nn.train(X_train, y_train, epochs=100)\n",
    "            accuracy = nn.accuracy(nn.predict(X_test), y_test)\n",
    "            print(f\"Layers: {layers}, Dropout: {dropout}, Accuracy: {accuracy}, Learning Rate: {learning}\")\n",
    "            best_HP[accuracy] = (layers, dropout, accuracy, learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for nn:\n",
      "Best layer: [65, 512, 256, 128, 5]\n",
      "Best dropout: 0.2\n",
      "Best accuracy: 0.8648\n",
      "Best learning rate: 0.1\n"
     ]
    }
   ],
   "source": [
    "max_key = max(best_HP)\n",
    "max_value = best_HP[max_key]\n",
    "\n",
    "\n",
    "print('Best hyperparameters for nn:')\n",
    "print(f\"Best layer: {max_value[0]}\")\n",
    "print(f\"Best dropout: {max_value[1]}\")\n",
    "print(f\"Best accuracy: {max_value[2]}\")\n",
    "print(f\"Best learning rate: {max_value[3]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# best parameter\n",
    "- Best layer: [65, 512, 256, 128, 5]\n",
    "- Best dropout: 0.2\n",
    "- Best accuracy: 0.8684\n",
    "- Best learning rate: 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.5586\n",
      "Epoch 10, Loss: 0.4288\n",
      "Epoch 20, Loss: 0.1846\n",
      "Epoch 30, Loss: 0.1854\n",
      "Epoch 40, Loss: 0.3843\n",
      "Epoch 50, Loss: 0.0779\n",
      "Epoch 60, Loss: 0.0492\n",
      "Epoch 70, Loss: 0.0599\n",
      "Epoch 80, Loss: 0.0224\n",
      "Epoch 90, Loss: 0.1113\n"
     ]
    }
   ],
   "source": [
    "df = np.load('../data/fashion_train.npy') \n",
    "\n",
    "df = pre.preprocess(df)\n",
    "\n",
    "best_hp_layers = [65, 512, 256, 128, 5]\n",
    "best_hp_dropout = 0.2\n",
    "best_hp_learning = 0.1\n",
    "\n",
    "X = df[:,:-1]\n",
    "y = df[:,-1]\n",
    "\n",
    "nn = NeuralNetwork(best_hp_layers, learning_rate=best_hp_learning, dropout_rate=best_hp_dropout)\n",
    "nn.train(X, y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./models/forward_feed_nn_model.pkl', 'wb') as f:\n",
    "    pickle.dump(nn, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 86.38%\n"
     ]
    }
   ],
   "source": [
    "#load test data\n",
    "\n",
    "df = np.load('../data/fashion_test.npy')\n",
    "\n",
    "df = pre.preprocess(df)\n",
    "\n",
    "X = df[:,:-1]\n",
    "y = df[:,-1]\n",
    "\n",
    "accuracy = nn.accuracy(nn.predict(X), y)\n",
    "print(f\"Final accuracy: {accuracy*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 86.44% ± 1.08%\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "model = pickle.load(open('models/forward_feed_nn_model.pkl', 'rb'))\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "acc_list = []\n",
    "\n",
    "for x in range(0, 1000):\n",
    "    sample = df.sample(1000, replace=True)\n",
    "    X = sample.iloc[:,:-1].to_numpy()\n",
    "    y = sample.iloc[:,-1].to_numpy()\n",
    "    preds = model.predict(X)\n",
    "    accuracy = np.sum(preds == y) / len(y)\n",
    "    acc_list.append(accuracy)\n",
    "\n",
    "print(f'accuracy: {round(np.mean(acc_list)*100, 2)}% ± {round(np.std(acc_list)*100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare to sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "import preprocessing as pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "df = np.load('../data/fashion_train.npy') \n",
    "\n",
    "df = pre.preprocess(df)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df[:,:-1], df[:,-1], test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunechristiansen/anaconda3/envs/my_project_env/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_layers = [256, 128]  # Hidden layer sizes\n",
    "output_size = 5  # Output layer size\n",
    "dropout_rate = 0.2  # Dropout rate\n",
    "learning_rate = 0.1  # Learning rate\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "\n",
    "# Input and first hidden layer\n",
    "model.add(Dense(hidden_layers[0], input_shape=(input_size,), activation='relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(Dense(hidden_layers[1], activation='relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(output_size, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=learning_rate),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7159 - loss: 1.0031 - val_accuracy: 0.8300 - val_loss: 0.4984\n",
      "Epoch 2/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7770 - loss: 0.6182 - val_accuracy: 0.8090 - val_loss: 0.5430\n",
      "Epoch 3/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7852 - loss: 0.6094 - val_accuracy: 0.8105 - val_loss: 0.5207\n",
      "Epoch 4/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7829 - loss: 0.6315 - val_accuracy: 0.8260 - val_loss: 0.4925\n",
      "Epoch 5/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7762 - loss: 0.6926 - val_accuracy: 0.8100 - val_loss: 0.6626\n",
      "Epoch 6/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7858 - loss: 0.7224 - val_accuracy: 0.7275 - val_loss: 1.1290\n",
      "Epoch 7/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7553 - loss: 0.9204 - val_accuracy: 0.7675 - val_loss: 0.8252\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',   \n",
    "    patience=3,           \n",
    "    restore_best_weights=True  \n",
    ")\n",
    "\n",
    "# EarlyStopping\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=100,                \n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping]  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 65) (5000,)\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8043 - loss: 0.5518\n",
      "[0.5467115044593811, 0.8080000281333923]\n"
     ]
    }
   ],
   "source": [
    "#Load test data\n",
    "df = np.load('../data/fashion_test.npy')\n",
    "\n",
    "df = pre.preprocess(df)\n",
    "\n",
    "x = df[:,:-1]\n",
    "y = df[:,-1]\n",
    "\n",
    "print(x.shape, y.shape) \n",
    "\n",
    "pred = model.predict(x)\n",
    "\n",
    "print(model.evaluate(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the accuracy and std for tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.85% ± 1.26%\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "df = pd.DataFrame(df)  # Convert to DataFrame if not already\n",
    "\n",
    "acc_list = []\n",
    "\n",
    "for _ in range(1000):  # Perform 1,000 bootstrapping iterations\n",
    "    # Bootstrap sampling\n",
    "    print(_)\n",
    "    sample = df.sample(1000, replace=True)\n",
    "    X = sample.iloc[:, :-1].to_numpy()  # Features\n",
    "    y = sample.iloc[:, -1].to_numpy()  # Labels\n",
    "    \n",
    "    # Predict and calculate accuracy\n",
    "    preds = model.predict(X)\n",
    "    preds = np.argmax(preds, axis=1)  # Convert probabilities to class labels\n",
    "    accuracy = np.sum(preds == y) / len(y)  # Compute accuracy\n",
    "    acc_list.append(accuracy)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "# Display the final mean accuracy and standard deviation\n",
    "mean_acc = np.mean(acc_list) * 100\n",
    "std_acc = np.std(acc_list) * 100\n",
    "print(f'Accuracy: {round(mean_acc, 2)}% ± {round(std_acc, 2)}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Even if the tensorflow model seems to have a exploding gradiant is it not over fitting, as the traning error is not lower then validation error, but the result of the accuracy i because tensorflow have a diffent implentaion, so we need to optimize the hyperparameter for tensorflow to get a good model and we also need to reduce the epoch because the exploding gradiant "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
