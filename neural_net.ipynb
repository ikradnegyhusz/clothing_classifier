{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_rows(arr):\n",
    "    row_means = arr.mean(axis=1, keepdims=True)  # Compute the mean of each row\n",
    "    row_stds = arr.std(axis=1, keepdims=True)    # Compute the std of each row\n",
    "    row_stds[row_stds == 0] = 1\n",
    "    standardized_arr = (arr - row_means) / row_stds  # Standardize each row\n",
    "    return standardized_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes, learning_rate=0.001, dropout_rate=0.5):\n",
    "        # Initialize parameters\n",
    "        self.layers = len(layer_sizes) - 1\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        # He initialization for weights\n",
    "        for i in range(self.layers):\n",
    "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2 / layer_sizes[i]))\n",
    "            self.biases.append(np.zeros((1, layer_sizes[i + 1])))\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        self.a = [X]\n",
    "        for i in range(self.layers - 1):\n",
    "            z = np.dot(self.a[-1], self.weights[i]) + self.biases[i]\n",
    "            a = self.relu(z)\n",
    "            # Apply dropout during training\n",
    "            if training:\n",
    "                dropout_mask = np.random.rand(*a.shape) > self.dropout_rate\n",
    "                a *= dropout_mask\n",
    "                a /= (1 - self.dropout_rate)\n",
    "            self.a.append(a)\n",
    "        z = np.dot(self.a[-1], self.weights[-1]) + self.biases[-1]\n",
    "        self.a.append(self.softmax(z))\n",
    "        return self.a[-1]\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        m = y.shape[0]\n",
    "        y_one_hot = np.eye(self.a[-1].shape[1])[y.astype(int)]\n",
    "\n",
    "        # Compute gradients for output layer\n",
    "        dz = self.a[-1] - y_one_hot\n",
    "        gradients_w = []\n",
    "        gradients_b = []\n",
    "\n",
    "        # Backpropagation through layers\n",
    "        for i in range(self.layers - 1, 0, -1):\n",
    "            dW = np.dot(self.a[i].T, dz) / m\n",
    "            db = np.sum(dz, axis=0, keepdims=True) / m\n",
    "            gradients_w.insert(0, dW)\n",
    "            gradients_b.insert(0, db)\n",
    "\n",
    "            dz = np.dot(dz, self.weights[i].T) * self.relu_derivative(self.a[i])\n",
    "\n",
    "        # Compute gradients for first layer\n",
    "        dW = np.dot(X.T, dz) / m\n",
    "        db = np.sum(dz, axis=0, keepdims=True) / m\n",
    "        gradients_w.insert(0, dW)\n",
    "        gradients_b.insert(0, db)\n",
    "\n",
    "        # Update weights and biases using gradients\n",
    "        for i in range(self.layers):\n",
    "            self.weights[i] -= self.learning_rate * gradients_w[i]\n",
    "            self.biases[i] -= self.learning_rate * gradients_b[i]\n",
    "\n",
    "    def train(self, X, y, epochs=100, batch_size=64):\n",
    "        for epoch in range(epochs):\n",
    "            # Mini-batch gradient descent\n",
    "            indices = np.arange(X.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X[indices[i:i + batch_size]]\n",
    "                y_batch = y[indices[i:i + batch_size]]\n",
    "                output = self.forward(X_batch)\n",
    "                self.backward(X_batch, y_batch)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                loss = -np.mean(np.log(output[range(y_batch.size), y_batch.astype(int)]))\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.forward(X, training=False)\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "    def accuracy(self, predictions, labels):\n",
    "        return np.mean(predictions == labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.1487\n",
      "Epoch 10, Loss: 1.3468\n",
      "Epoch 20, Loss: 1.3177\n",
      "Epoch 30, Loss: 1.4312\n",
      "Epoch 40, Loss: 1.1307\n",
      "Epoch 50, Loss: 1.1376\n",
      "Epoch 60, Loss: 0.8508\n",
      "Epoch 70, Loss: 0.6662\n",
      "Epoch 80, Loss: 0.7720\n",
      "Epoch 90, Loss: 0.6219\n",
      "Epoch 100, Loss: 1.2581\n",
      "Epoch 110, Loss: 0.8568\n",
      "Epoch 120, Loss: 1.0584\n",
      "Epoch 130, Loss: 0.5446\n",
      "Epoch 140, Loss: 0.6673\n",
      "Epoch 150, Loss: 0.7779\n",
      "Epoch 160, Loss: 0.7975\n",
      "Epoch 170, Loss: 0.8380\n",
      "Epoch 180, Loss: 0.5429\n",
      "Epoch 190, Loss: 0.4949\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "train = np.load(\"data/fashion_train.npy\")\n",
    "X_train = train[:, :-1]\n",
    "y_train = train[:, -1]\n",
    "\n",
    "# Standardize the data\n",
    "X_train = standardize_rows(X_train)\n",
    "\n",
    "# Initialize neural network with multiple layers\n",
    "layer_sizes = [784, 256, 128, 64, 32, 5]\n",
    "nn = NeuralNetwork(layer_sizes, learning_rate=0.001, dropout_rate=0.3)\n",
    "nn.train(X_train, y_train, epochs=200, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 74.10%\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test = np.load(\"data/fashion_test.npy\")\n",
    "X_test = test[:, :-1]\n",
    "y_test = test[:, -1]\n",
    "X_test = standardize_rows(X_test)\n",
    "\n",
    "# Test the network\n",
    "predictions = nn.predict(X_test)\n",
    "accuracy = nn.accuracy(predictions, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
