{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_rows(arr):\n",
    "    row_means = arr.mean(axis=1, keepdims=True)  # Compute the mean of each row\n",
    "    row_stds = arr.std(axis=1, keepdims=True)    # Compute the std of each row\n",
    "    row_stds[row_stds == 0] = 1\n",
    "    standardized_arr = (arr - row_means) / row_stds  # Standardize each row\n",
    "    return standardized_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes, learning_rate=0.001, dropout_rate=0.5):\n",
    "        # Initialize parameters\n",
    "        self.layers = len(layer_sizes) - 1\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        # He initialization for weights\n",
    "        for i in range(self.layers):\n",
    "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2 / layer_sizes[i]))\n",
    "            self.biases.append(np.zeros((1, layer_sizes[i + 1])))\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        self.a = [X]\n",
    "        for i in range(self.layers - 1):\n",
    "            z = np.dot(self.a[-1], self.weights[i]) + self.biases[i]\n",
    "            a = self.relu(z)\n",
    "\n",
    "            # Apply dropout only during training\n",
    "            if training:\n",
    "                dropout_mask = np.random.rand(*a.shape) > self.dropout_rate\n",
    "                a *= dropout_mask\n",
    "                a /= (1 - self.dropout_rate)\n",
    "\n",
    "            self.a.append(a)\n",
    "\n",
    "        z = np.dot(self.a[-1], self.weights[-1]) + self.biases[-1]\n",
    "        self.a.append(self.softmax(z))\n",
    "        return self.a[-1]\n",
    "\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        m = y.shape[0]\n",
    "        y_one_hot = np.eye(self.a[-1].shape[1])[y.astype(int)]\n",
    "\n",
    "        # Compute gradients for output layer\n",
    "        dz = self.a[-1] - y_one_hot\n",
    "        gradients_w = []\n",
    "        gradients_b = []\n",
    "\n",
    "        # Backpropagation through layers\n",
    "        for i in range(self.layers - 1, 0, -1):\n",
    "            dW = np.dot(self.a[i].T, dz) / m\n",
    "            db = np.sum(dz, axis=0, keepdims=True) / m\n",
    "            gradients_w.insert(0, dW)\n",
    "            gradients_b.insert(0, db)\n",
    "\n",
    "            dz = np.dot(dz, self.weights[i].T) * self.relu_derivative(self.a[i])\n",
    "\n",
    "        # Compute gradients for first layer\n",
    "        dW = np.dot(X.T, dz) / m\n",
    "        db = np.sum(dz, axis=0, keepdims=True) / m\n",
    "        gradients_w.insert(0, dW)\n",
    "        gradients_b.insert(0, db)\n",
    "\n",
    "        # Update weights and biases using gradients\n",
    "        for i in range(self.layers):\n",
    "            self.weights[i] -= self.learning_rate * gradients_w[i]\n",
    "            self.biases[i] -= self.learning_rate * gradients_b[i]\n",
    "\n",
    "    def train(self, X, y, epochs=100, batch_size=64):\n",
    "        for epoch in range(epochs):\n",
    "            # Mini-batch gradient descent\n",
    "            indices = np.arange(X.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X[indices[i:i + batch_size]]\n",
    "                y_batch = y[indices[i:i + batch_size]]\n",
    "                output = self.forward(X_batch)\n",
    "                self.backward(X_batch, y_batch)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                loss = -np.mean(np.log(output[range(y_batch.size), y_batch.astype(int)]))\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.forward(X, training=False)\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "    def accuracy(self, predictions, labels):\n",
    "        return np.mean(predictions == labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train = np.load(\"data/fashion_train.npy\")\n",
    "X_train = train[:, :-1]\n",
    "y_train = train[:, -1]\n",
    "\n",
    "# Standardize the data\n",
    "X_train = standardize_rows(X_train)\n",
    "\n",
    "# Load test data\n",
    "test = np.load(\"data/fashion_test.npy\")\n",
    "X_test = test[:, :-1]\n",
    "y_test = test[:, -1]\n",
    "X_test = standardize_rows(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers: [784, 128, 5], dropout: 0.2\n",
      "Epoch 0, Loss: 1.2920\n",
      "Epoch 10, Loss: 0.5831\n",
      "Epoch 20, Loss: 0.4275\n",
      "Epoch 30, Loss: 0.3334\n",
      "Epoch 40, Loss: 0.4194\n",
      "Epoch 50, Loss: 0.5225\n",
      "Epoch 60, Loss: 0.2807\n",
      "Epoch 70, Loss: 0.8202\n",
      "Epoch 80, Loss: 0.6499\n",
      "Epoch 90, Loss: 0.4382\n",
      "Layers: [784, 128, 5], Dropout: 0.2, Accuracy: 0.8358\n",
      "layers: [784, 128, 5], dropout: 0.3\n",
      "Epoch 0, Loss: 0.8200\n",
      "Epoch 10, Loss: 0.4906\n",
      "Epoch 20, Loss: 0.4412\n",
      "Epoch 30, Loss: 0.2982\n",
      "Epoch 40, Loss: 0.1987\n",
      "Epoch 50, Loss: 0.5179\n",
      "Epoch 60, Loss: 0.4995\n",
      "Epoch 70, Loss: 0.4942\n",
      "Epoch 80, Loss: 0.3499\n",
      "Epoch 90, Loss: 0.4869\n",
      "Layers: [784, 128, 5], Dropout: 0.3, Accuracy: 0.834\n",
      "layers: [784, 128, 5], dropout: 0.5\n",
      "Epoch 0, Loss: 1.7099\n",
      "Epoch 10, Loss: 0.7919\n",
      "Epoch 20, Loss: 0.7298\n",
      "Epoch 30, Loss: 0.3157\n",
      "Epoch 40, Loss: 0.5039\n",
      "Epoch 50, Loss: 0.3266\n",
      "Epoch 60, Loss: 0.3879\n",
      "Epoch 70, Loss: 0.3028\n",
      "Epoch 80, Loss: 0.1298\n",
      "Epoch 90, Loss: 0.2467\n",
      "Layers: [784, 128, 5], Dropout: 0.5, Accuracy: 0.8158\n",
      "layers: [784, 256, 128, 5], dropout: 0.2\n",
      "Epoch 0, Loss: 1.1972\n",
      "Epoch 10, Loss: 0.8050\n",
      "Epoch 20, Loss: 0.3703\n",
      "Epoch 30, Loss: 0.4073\n",
      "Epoch 40, Loss: 0.7189\n",
      "Epoch 50, Loss: 0.3333\n",
      "Epoch 60, Loss: 0.2152\n",
      "Epoch 70, Loss: 0.7852\n",
      "Epoch 80, Loss: 0.3369\n",
      "Epoch 90, Loss: 0.5236\n",
      "Layers: [784, 256, 128, 5], Dropout: 0.2, Accuracy: 0.8322\n",
      "layers: [784, 256, 128, 5], dropout: 0.3\n",
      "Epoch 0, Loss: 1.3174\n",
      "Epoch 10, Loss: 0.6182\n",
      "Epoch 20, Loss: 0.6660\n",
      "Epoch 30, Loss: 0.8542\n",
      "Epoch 40, Loss: 0.3567\n",
      "Epoch 50, Loss: 0.5871\n",
      "Epoch 60, Loss: 0.5785\n",
      "Epoch 70, Loss: 0.8071\n",
      "Epoch 80, Loss: 0.6857\n",
      "Epoch 90, Loss: 0.5392\n",
      "Layers: [784, 256, 128, 5], Dropout: 0.3, Accuracy: 0.8214\n",
      "layers: [784, 256, 128, 5], dropout: 0.5\n",
      "Epoch 0, Loss: 2.4640\n",
      "Epoch 10, Loss: 1.3237\n",
      "Epoch 20, Loss: 0.5633\n",
      "Epoch 30, Loss: 0.5984\n",
      "Epoch 40, Loss: 0.4310\n",
      "Epoch 50, Loss: 0.8106\n",
      "Epoch 60, Loss: 0.5243\n",
      "Epoch 70, Loss: 0.7293\n",
      "Epoch 80, Loss: 0.8186\n",
      "Epoch 90, Loss: 0.5011\n",
      "Layers: [784, 256, 128, 5], Dropout: 0.5, Accuracy: 0.8\n",
      "layers: [784, 512, 256, 128, 5], dropout: 0.2\n",
      "Epoch 0, Loss: 1.0602\n",
      "Epoch 10, Loss: 0.7741\n",
      "Epoch 20, Loss: 0.5812\n",
      "Epoch 30, Loss: 0.3843\n",
      "Epoch 40, Loss: 0.2086\n",
      "Epoch 50, Loss: 0.9272\n",
      "Epoch 60, Loss: 0.9293\n",
      "Epoch 70, Loss: 0.7136\n",
      "Epoch 80, Loss: 0.1103\n",
      "Epoch 90, Loss: 0.8996\n",
      "Layers: [784, 512, 256, 128, 5], Dropout: 0.2, Accuracy: 0.8344\n",
      "layers: [784, 512, 256, 128, 5], dropout: 0.3\n",
      "Epoch 0, Loss: 1.5457\n",
      "Epoch 10, Loss: 1.0551\n",
      "Epoch 20, Loss: 0.6896\n",
      "Epoch 30, Loss: 0.3086\n",
      "Epoch 40, Loss: 0.6670\n",
      "Epoch 50, Loss: 0.7628\n",
      "Epoch 60, Loss: 0.4794\n",
      "Epoch 70, Loss: 0.8789\n",
      "Epoch 80, Loss: 0.4179\n",
      "Epoch 90, Loss: 0.6584\n",
      "Layers: [784, 512, 256, 128, 5], Dropout: 0.3, Accuracy: 0.8188\n",
      "layers: [784, 512, 256, 128, 5], dropout: 0.5\n",
      "Epoch 0, Loss: 2.2303\n",
      "Epoch 10, Loss: 1.2331\n",
      "Epoch 20, Loss: 1.4874\n",
      "Epoch 30, Loss: 0.8325\n",
      "Epoch 40, Loss: 0.9124\n",
      "Epoch 50, Loss: 1.3010\n",
      "Epoch 60, Loss: 0.7468\n",
      "Epoch 70, Loss: 0.6035\n",
      "Epoch 80, Loss: 0.7573\n",
      "Epoch 90, Loss: 0.8713\n",
      "Layers: [784, 512, 256, 128, 5], Dropout: 0.5, Accuracy: 0.7298\n"
     ]
    }
   ],
   "source": [
    "layer_options = [[784, 128, 5], [784, 256, 128, 5], [784, 512, 256, 128, 5]]\n",
    "dropout_rates = [0.2, 0.3, 0.5]\n",
    "\n",
    "for layers in layer_options:\n",
    "    for dropout in dropout_rates:\n",
    "        nn = NeuralNetwork(layers, learning_rate=0.001, dropout_rate=dropout)\n",
    "        nn.train(X_train, y_train, epochs=100)\n",
    "        accuracy = nn.accuracy(nn.predict(X_test), y_test)\n",
    "        print(f\"Layers: {layers}, Dropout: {dropout}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 80.10%\n"
     ]
    }
   ],
   "source": [
    "# Test the network\n",
    "predictions = nn.predict(X_test)\n",
    "accuracy = nn.accuracy(predictions, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
