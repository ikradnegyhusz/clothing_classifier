{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_rows(arr):\n",
    "    row_means = arr.mean(axis=1, keepdims=True)  # Compute the mean of each row\n",
    "    row_stds = arr.std(axis=1, keepdims=True)    # Compute the std of each row\n",
    "    row_stds[row_stds == 0] = 1\n",
    "    standardized_arr = (arr - row_means) / row_stds  # Standardize each row\n",
    "    return standardized_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes, learning_rate=0.01):\n",
    "        # Initialize network parameters\n",
    "        self.layers = len(layer_sizes) - 1\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        # Initialize weights and biases for each layer\n",
    "        for i in range(self.layers):\n",
    "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]))\n",
    "            self.biases.append(np.zeros((1, layer_sizes[i + 1])))\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_derivative(self, z):\n",
    "        return z * (1 - z)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.a = [X]\n",
    "        for i in range(self.layers - 1):\n",
    "            z = np.dot(self.a[-1], self.weights[i]) + self.biases[i]\n",
    "            self.a.append(self.sigmoid(z))\n",
    "        z = np.dot(self.a[-1], self.weights[-1]) + self.biases[-1]\n",
    "        self.a.append(self.softmax(z))\n",
    "        return self.a[-1]\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        m = y.shape[0]\n",
    "        y_one_hot = np.eye(self.a[-1].shape[1])[y.astype(int)]\n",
    "\n",
    "        # Compute gradients for the output layer\n",
    "        dz = self.a[-1] - y_one_hot\n",
    "        gradients_w = []\n",
    "        gradients_b = []\n",
    "\n",
    "        # Backpropagation through hidden layers\n",
    "        for i in range(self.layers - 1, 0, -1):\n",
    "            dW = np.dot(self.a[i].T, dz) / m\n",
    "            db = np.sum(dz, axis=0, keepdims=True) / m\n",
    "            gradients_w.insert(0, dW)\n",
    "            gradients_b.insert(0, db)\n",
    "\n",
    "            # **Fix**: Use correct activations for derivatives\n",
    "            dz = np.dot(dz, self.weights[i].T) * self.sigmoid_derivative(self.a[i])\n",
    "\n",
    "        # Compute gradients for the first layer\n",
    "        dW = np.dot(X.T, dz) / m\n",
    "        db = np.sum(dz, axis=0, keepdims=True) / m\n",
    "        gradients_w.insert(0, dW)\n",
    "        gradients_b.insert(0, db)\n",
    "\n",
    "        # Update weights and biases\n",
    "        for i in range(self.layers):\n",
    "            self.weights[i] -= self.learning_rate * gradients_w[i]\n",
    "            self.biases[i] -= self.learning_rate * gradients_b[i]\n",
    "\n",
    "\n",
    "    def train(self, X, y, epochs=100):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y)\n",
    "            if epoch % 10 == 0:\n",
    "                loss = -np.mean(np.log(output[range(y.size), y.astype(int)]))\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.forward(X)\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "    def accuracy(self, predictions, labels):\n",
    "        return np.mean(predictions == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 10.2574\n",
      "Epoch 10, Loss: 6.9561\n",
      "Epoch 20, Loss: 4.0802\n",
      "Epoch 30, Loss: 2.4878\n",
      "Epoch 40, Loss: 1.9265\n",
      "Epoch 50, Loss: 1.7190\n",
      "Epoch 60, Loss: 1.6321\n",
      "Epoch 70, Loss: 1.5853\n",
      "Epoch 80, Loss: 1.5523\n",
      "Epoch 90, Loss: 1.5248\n",
      "Epoch 100, Loss: 1.5001\n",
      "Epoch 110, Loss: 1.4773\n",
      "Epoch 120, Loss: 1.4560\n",
      "Epoch 130, Loss: 1.4359\n",
      "Epoch 140, Loss: 1.4169\n",
      "Epoch 150, Loss: 1.3990\n",
      "Epoch 160, Loss: 1.3820\n",
      "Epoch 170, Loss: 1.3658\n",
      "Epoch 180, Loss: 1.3503\n",
      "Epoch 190, Loss: 1.3356\n",
      "Epoch 200, Loss: 1.3214\n",
      "Epoch 210, Loss: 1.3079\n",
      "Epoch 220, Loss: 1.2949\n",
      "Epoch 230, Loss: 1.2824\n",
      "Epoch 240, Loss: 1.2703\n",
      "Epoch 250, Loss: 1.2587\n",
      "Epoch 260, Loss: 1.2474\n",
      "Epoch 270, Loss: 1.2365\n",
      "Epoch 280, Loss: 1.2259\n",
      "Epoch 290, Loss: 1.2157\n",
      "Epoch 300, Loss: 1.2058\n",
      "Epoch 310, Loss: 1.1961\n",
      "Epoch 320, Loss: 1.1867\n",
      "Epoch 330, Loss: 1.1776\n",
      "Epoch 340, Loss: 1.1687\n",
      "Epoch 350, Loss: 1.1600\n",
      "Epoch 360, Loss: 1.1516\n",
      "Epoch 370, Loss: 1.1433\n",
      "Epoch 380, Loss: 1.1353\n",
      "Epoch 390, Loss: 1.1274\n",
      "Epoch 400, Loss: 1.1198\n",
      "Epoch 410, Loss: 1.1123\n",
      "Epoch 420, Loss: 1.1051\n",
      "Epoch 430, Loss: 1.0979\n",
      "Epoch 440, Loss: 1.0910\n",
      "Epoch 450, Loss: 1.0842\n",
      "Epoch 460, Loss: 1.0776\n",
      "Epoch 470, Loss: 1.0711\n",
      "Epoch 480, Loss: 1.0648\n",
      "Epoch 490, Loss: 1.0586\n",
      "Epoch 500, Loss: 1.0526\n",
      "Epoch 510, Loss: 1.0467\n",
      "Epoch 520, Loss: 1.0409\n",
      "Epoch 530, Loss: 1.0352\n",
      "Epoch 540, Loss: 1.0297\n",
      "Epoch 550, Loss: 1.0243\n",
      "Epoch 560, Loss: 1.0190\n",
      "Epoch 570, Loss: 1.0138\n",
      "Epoch 580, Loss: 1.0087\n",
      "Epoch 590, Loss: 1.0038\n",
      "Epoch 600, Loss: 0.9989\n",
      "Epoch 610, Loss: 0.9941\n",
      "Epoch 620, Loss: 0.9895\n",
      "Epoch 630, Loss: 0.9849\n",
      "Epoch 640, Loss: 0.9804\n",
      "Epoch 650, Loss: 0.9760\n",
      "Epoch 660, Loss: 0.9716\n",
      "Epoch 670, Loss: 0.9674\n",
      "Epoch 680, Loss: 0.9632\n",
      "Epoch 690, Loss: 0.9591\n",
      "Epoch 700, Loss: 0.9551\n",
      "Epoch 710, Loss: 0.9512\n",
      "Epoch 720, Loss: 0.9473\n",
      "Epoch 730, Loss: 0.9435\n",
      "Epoch 740, Loss: 0.9398\n",
      "Epoch 750, Loss: 0.9361\n",
      "Epoch 760, Loss: 0.9326\n",
      "Epoch 770, Loss: 0.9290\n",
      "Epoch 780, Loss: 0.9256\n",
      "Epoch 790, Loss: 0.9222\n",
      "Epoch 800, Loss: 0.9188\n",
      "Epoch 810, Loss: 0.9156\n",
      "Epoch 820, Loss: 0.9123\n",
      "Epoch 830, Loss: 0.9092\n",
      "Epoch 840, Loss: 0.9061\n",
      "Epoch 850, Loss: 0.9030\n",
      "Epoch 860, Loss: 0.9000\n",
      "Epoch 870, Loss: 0.8970\n",
      "Epoch 880, Loss: 0.8941\n",
      "Epoch 890, Loss: 0.8913\n",
      "Epoch 900, Loss: 0.8884\n",
      "Epoch 910, Loss: 0.8857\n",
      "Epoch 920, Loss: 0.8829\n",
      "Epoch 930, Loss: 0.8803\n",
      "Epoch 940, Loss: 0.8776\n",
      "Epoch 950, Loss: 0.8750\n",
      "Epoch 960, Loss: 0.8725\n",
      "Epoch 970, Loss: 0.8699\n",
      "Epoch 980, Loss: 0.8675\n",
      "Epoch 990, Loss: 0.8650\n",
      "Epoch 1000, Loss: 0.8626\n",
      "Epoch 1010, Loss: 0.8602\n",
      "Epoch 1020, Loss: 0.8579\n",
      "Epoch 1030, Loss: 0.8556\n",
      "Epoch 1040, Loss: 0.8533\n",
      "Epoch 1050, Loss: 0.8511\n",
      "Epoch 1060, Loss: 0.8489\n",
      "Epoch 1070, Loss: 0.8467\n",
      "Epoch 1080, Loss: 0.8446\n",
      "Epoch 1090, Loss: 0.8425\n",
      "Epoch 1100, Loss: 0.8404\n",
      "Epoch 1110, Loss: 0.8383\n",
      "Epoch 1120, Loss: 0.8363\n",
      "Epoch 1130, Loss: 0.8343\n",
      "Epoch 1140, Loss: 0.8323\n",
      "Epoch 1150, Loss: 0.8304\n",
      "Epoch 1160, Loss: 0.8285\n",
      "Epoch 1170, Loss: 0.8266\n",
      "Epoch 1180, Loss: 0.8247\n",
      "Epoch 1190, Loss: 0.8229\n",
      "Epoch 1200, Loss: 0.8210\n",
      "Epoch 1210, Loss: 0.8192\n",
      "Epoch 1220, Loss: 0.8175\n",
      "Epoch 1230, Loss: 0.8157\n",
      "Epoch 1240, Loss: 0.8140\n",
      "Epoch 1250, Loss: 0.8123\n",
      "Epoch 1260, Loss: 0.8106\n",
      "Epoch 1270, Loss: 0.8089\n",
      "Epoch 1280, Loss: 0.8072\n",
      "Epoch 1290, Loss: 0.8056\n",
      "Epoch 1300, Loss: 0.8040\n",
      "Epoch 1310, Loss: 0.8024\n",
      "Epoch 1320, Loss: 0.8008\n",
      "Epoch 1330, Loss: 0.7992\n",
      "Epoch 1340, Loss: 0.7977\n",
      "Epoch 1350, Loss: 0.7962\n",
      "Epoch 1360, Loss: 0.7946\n",
      "Epoch 1370, Loss: 0.7931\n",
      "Epoch 1380, Loss: 0.7917\n",
      "Epoch 1390, Loss: 0.7902\n",
      "Epoch 1400, Loss: 0.7887\n",
      "Epoch 1410, Loss: 0.7873\n",
      "Epoch 1420, Loss: 0.7859\n",
      "Epoch 1430, Loss: 0.7845\n",
      "Epoch 1440, Loss: 0.7831\n",
      "Epoch 1450, Loss: 0.7817\n",
      "Epoch 1460, Loss: 0.7803\n",
      "Epoch 1470, Loss: 0.7790\n",
      "Epoch 1480, Loss: 0.7776\n",
      "Epoch 1490, Loss: 0.7763\n",
      "Epoch 1500, Loss: 0.7750\n",
      "Epoch 1510, Loss: 0.7737\n",
      "Epoch 1520, Loss: 0.7724\n",
      "Epoch 1530, Loss: 0.7711\n",
      "Epoch 1540, Loss: 0.7699\n",
      "Epoch 1550, Loss: 0.7686\n",
      "Epoch 1560, Loss: 0.7674\n",
      "Epoch 1570, Loss: 0.7661\n",
      "Epoch 1580, Loss: 0.7649\n",
      "Epoch 1590, Loss: 0.7637\n",
      "Epoch 1600, Loss: 0.7625\n",
      "Epoch 1610, Loss: 0.7613\n",
      "Epoch 1620, Loss: 0.7602\n",
      "Epoch 1630, Loss: 0.7590\n",
      "Epoch 1640, Loss: 0.7578\n",
      "Epoch 1650, Loss: 0.7567\n",
      "Epoch 1660, Loss: 0.7555\n",
      "Epoch 1670, Loss: 0.7544\n",
      "Epoch 1680, Loss: 0.7533\n",
      "Epoch 1690, Loss: 0.7522\n",
      "Epoch 1700, Loss: 0.7511\n",
      "Epoch 1710, Loss: 0.7500\n",
      "Epoch 1720, Loss: 0.7489\n",
      "Epoch 1730, Loss: 0.7478\n",
      "Epoch 1740, Loss: 0.7468\n",
      "Epoch 1750, Loss: 0.7457\n",
      "Epoch 1760, Loss: 0.7447\n",
      "Epoch 1770, Loss: 0.7436\n",
      "Epoch 1780, Loss: 0.7426\n",
      "Epoch 1790, Loss: 0.7416\n",
      "Epoch 1800, Loss: 0.7406\n",
      "Epoch 1810, Loss: 0.7396\n",
      "Epoch 1820, Loss: 0.7386\n",
      "Epoch 1830, Loss: 0.7376\n",
      "Epoch 1840, Loss: 0.7366\n",
      "Epoch 1850, Loss: 0.7356\n",
      "Epoch 1860, Loss: 0.7346\n",
      "Epoch 1870, Loss: 0.7336\n",
      "Epoch 1880, Loss: 0.7327\n",
      "Epoch 1890, Loss: 0.7317\n",
      "Epoch 1900, Loss: 0.7308\n",
      "Epoch 1910, Loss: 0.7299\n",
      "Epoch 1920, Loss: 0.7289\n",
      "Epoch 1930, Loss: 0.7280\n",
      "Epoch 1940, Loss: 0.7271\n",
      "Epoch 1950, Loss: 0.7262\n",
      "Epoch 1960, Loss: 0.7252\n",
      "Epoch 1970, Loss: 0.7243\n",
      "Epoch 1980, Loss: 0.7235\n",
      "Epoch 1990, Loss: 0.7226\n",
      "Epoch 2000, Loss: 0.7217\n",
      "Epoch 2010, Loss: 0.7208\n",
      "Epoch 2020, Loss: 0.7199\n",
      "Epoch 2030, Loss: 0.7191\n",
      "Epoch 2040, Loss: 0.7182\n",
      "Epoch 2050, Loss: 0.7173\n",
      "Epoch 2060, Loss: 0.7165\n",
      "Epoch 2070, Loss: 0.7157\n",
      "Epoch 2080, Loss: 0.7148\n",
      "Epoch 2090, Loss: 0.7140\n",
      "Epoch 2100, Loss: 0.7132\n",
      "Epoch 2110, Loss: 0.7123\n",
      "Epoch 2120, Loss: 0.7115\n",
      "Epoch 2130, Loss: 0.7107\n",
      "Epoch 2140, Loss: 0.7099\n",
      "Epoch 2150, Loss: 0.7091\n",
      "Epoch 2160, Loss: 0.7083\n",
      "Epoch 2170, Loss: 0.7075\n",
      "Epoch 2180, Loss: 0.7067\n",
      "Epoch 2190, Loss: 0.7060\n",
      "Epoch 2200, Loss: 0.7052\n",
      "Epoch 2210, Loss: 0.7044\n",
      "Epoch 2220, Loss: 0.7037\n",
      "Epoch 2230, Loss: 0.7029\n",
      "Epoch 2240, Loss: 0.7021\n",
      "Epoch 2250, Loss: 0.7014\n",
      "Epoch 2260, Loss: 0.7006\n",
      "Epoch 2270, Loss: 0.6999\n",
      "Epoch 2280, Loss: 0.6992\n",
      "Epoch 2290, Loss: 0.6984\n",
      "Epoch 2300, Loss: 0.6977\n",
      "Epoch 2310, Loss: 0.6970\n",
      "Epoch 2320, Loss: 0.6963\n",
      "Epoch 2330, Loss: 0.6956\n",
      "Epoch 2340, Loss: 0.6949\n",
      "Epoch 2350, Loss: 0.6942\n",
      "Epoch 2360, Loss: 0.6935\n",
      "Epoch 2370, Loss: 0.6928\n",
      "Epoch 2380, Loss: 0.6921\n",
      "Epoch 2390, Loss: 0.6914\n",
      "Epoch 2400, Loss: 0.6907\n",
      "Epoch 2410, Loss: 0.6900\n",
      "Epoch 2420, Loss: 0.6894\n",
      "Epoch 2430, Loss: 0.6887\n",
      "Epoch 2440, Loss: 0.6880\n",
      "Epoch 2450, Loss: 0.6874\n",
      "Epoch 2460, Loss: 0.6867\n",
      "Epoch 2470, Loss: 0.6861\n",
      "Epoch 2480, Loss: 0.6854\n",
      "Epoch 2490, Loss: 0.6848\n",
      "Epoch 2500, Loss: 0.6841\n",
      "Epoch 2510, Loss: 0.6835\n",
      "Epoch 2520, Loss: 0.6829\n",
      "Epoch 2530, Loss: 0.6823\n",
      "Epoch 2540, Loss: 0.6816\n",
      "Epoch 2550, Loss: 0.6810\n",
      "Epoch 2560, Loss: 0.6804\n",
      "Epoch 2570, Loss: 0.6798\n",
      "Epoch 2580, Loss: 0.6792\n",
      "Epoch 2590, Loss: 0.6786\n",
      "Epoch 2600, Loss: 0.6780\n",
      "Epoch 2610, Loss: 0.6774\n",
      "Epoch 2620, Loss: 0.6768\n",
      "Epoch 2630, Loss: 0.6762\n",
      "Epoch 2640, Loss: 0.6756\n",
      "Epoch 2650, Loss: 0.6750\n",
      "Epoch 2660, Loss: 0.6745\n",
      "Epoch 2670, Loss: 0.6739\n",
      "Epoch 2680, Loss: 0.6733\n",
      "Epoch 2690, Loss: 0.6727\n",
      "Epoch 2700, Loss: 0.6722\n",
      "Epoch 2710, Loss: 0.6716\n",
      "Epoch 2720, Loss: 0.6711\n",
      "Epoch 2730, Loss: 0.6705\n",
      "Epoch 2740, Loss: 0.6700\n",
      "Epoch 2750, Loss: 0.6694\n",
      "Epoch 2760, Loss: 0.6689\n",
      "Epoch 2770, Loss: 0.6683\n",
      "Epoch 2780, Loss: 0.6678\n",
      "Epoch 2790, Loss: 0.6672\n",
      "Epoch 2800, Loss: 0.6667\n",
      "Epoch 2810, Loss: 0.6662\n",
      "Epoch 2820, Loss: 0.6657\n",
      "Epoch 2830, Loss: 0.6651\n",
      "Epoch 2840, Loss: 0.6646\n",
      "Epoch 2850, Loss: 0.6641\n",
      "Epoch 2860, Loss: 0.6636\n",
      "Epoch 2870, Loss: 0.6631\n",
      "Epoch 2880, Loss: 0.6626\n",
      "Epoch 2890, Loss: 0.6620\n",
      "Epoch 2900, Loss: 0.6615\n",
      "Epoch 2910, Loss: 0.6610\n",
      "Epoch 2920, Loss: 0.6605\n",
      "Epoch 2930, Loss: 0.6600\n",
      "Epoch 2940, Loss: 0.6596\n",
      "Epoch 2950, Loss: 0.6591\n",
      "Epoch 2960, Loss: 0.6586\n",
      "Epoch 2970, Loss: 0.6581\n",
      "Epoch 2980, Loss: 0.6576\n",
      "Epoch 2990, Loss: 0.6571\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "train = np.load(\"data/fashion_train.npy\")\n",
    "X_train = train[:, :-1]\n",
    "y_train = train[:, -1]\n",
    "\n",
    "# Standardize the data\n",
    "X_train = standardize_rows(X_train)\n",
    "\n",
    "# Initialize neural network with multiple layers\n",
    "layer_sizes = [X_train.shape[1], 128, 64, 32, len(np.unique(y_train))]\n",
    "nn = NeuralNetwork(layer_sizes, learning_rate=0.02)\n",
    "\n",
    "# Train the network\n",
    "nn.train(X_train, y_train, epochs=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 71.42%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load test data\n",
    "test = np.load(\"data/fashion_test.npy\")\n",
    "X_test = test[:, :-1]\n",
    "y_test = test[:, -1]\n",
    "X_test = standardize_rows(X_test)\n",
    "\n",
    "# Test the network\n",
    "predictions = nn.predict(X_test)\n",
    "accuracy = nn.accuracy(predictions, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
