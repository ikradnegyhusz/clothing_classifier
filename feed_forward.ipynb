{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "train = np.load('./data/fashion_train.npy')\n",
    "\n",
    "train_test = train[:, : -1]\n",
    "y = train[:,-1]\n",
    "\n",
    "print(train_test.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def input_layer(X_train, y_train):\n",
    "    n_feartures = X_train.shape[1]\n",
    "    output_nodes = len(set(y_train))\n",
    "    len_of_X_train = len(X_train)\n",
    "    return n_feartures, output_nodes, len_of_X_train\n",
    "\n",
    "\n",
    "def hidden_layer(X, n_feartures, nunbers_of_nodes_hidden, len_of_X_train):\n",
    "    #first hidden layer\n",
    "\n",
    "    W = np.random.randn(n_feartures, nunbers_of_nodes_hidden)\n",
    "\n",
    "    b = np.random.randn(len_of_X_train, nunbers_of_nodes_hidden)\n",
    "\n",
    "    z = X@W+b\n",
    "\n",
    "    a = np.maximum(0, z)\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "#output layer\n",
    "def output_layer(a, output_nodes, len_of_X_train):\n",
    "\n",
    "    W = np.random.randn(a.shape[1], output_nodes)\n",
    "\n",
    "    b = np.random.randn(len_of_X_train, output_nodes)\n",
    "\n",
    "    z = a@W+b\n",
    "\n",
    "    softmax = np.exp(z - np.max(z))/np.sum(np.exp(z - np.max(z)))\n",
    "    preds = np.argmax(softmax, axis=1)\n",
    "\n",
    "    return print(f'pred: {np.argmax(softmax, axis=1)}, shape: {softmax.shape}'), preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: [0 0 0 ... 0 0 0], shape: (10000, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, array([0, 0, 0, ..., 0, 0, 0], dtype=int64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_test\n",
    "\n",
    "n_feartures, output_nodes, len_of_X_train = input_layer(X, y)\n",
    "\n",
    "a = hidden_layer(X, n_feartures, nunbers_of_nodes_hidden=40, len_of_X_train=len_of_X_train)\n",
    "\n",
    "preds = output_layer(a, output_nodes, len_of_X_train)\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return np.exp(x) / (np.exp(x) + 1)\n",
    "\n",
    "def derivative_sigmoid(x):\n",
    "    sig = sigmoid(x)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "def softmax(z):\n",
    "    return np.exp(z - np.max(z))/np.sum(np.exp(z - np.max(z)))\n",
    "\n",
    "def softmax_derivative(z):\n",
    "    s = np.exp(z) / np.sum(np.exp(z))\n",
    "    s = s.reshape(-1, 1)\n",
    "    jacobian = np.diagflat(s) - np.dot(s, s.T)\n",
    "    return jacobian\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def derivative_relu(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def cross_entropy(y, a, epsilon=1e-12):\n",
    "    a = np.clip(a, epsilon, 1. - epsilon)\n",
    "    return -np.sum(y * np.log(a))\n",
    "\n",
    "def cross_entropy_derivative_a(y, a):\n",
    "    return -1/a * np.sum(y)\n",
    "\n",
    "def cross_entropy_derivative_softmax(y, a):\n",
    "    return a-y\n",
    "\n",
    "\n",
    "def binary_cross_entropy(y, a, epsilon=1e-12):\n",
    "    a = np.clip(a, epsilon, 1. - epsilon)  # Avoid log(0) errors\n",
    "    return -np.mean(y * np.log(a) + (1 - y) * np.log(1 - a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative_dict = {\n",
    "    softmax:softmax_derivative,\n",
    "    cross_entropy:cross_entropy_derivative_a,\n",
    "    sigmoid:derivative_sigmoid,\n",
    "    relu:derivative_relu\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andra\\AppData\\Local\\Temp\\ipykernel_6776\\3473699835.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(x) / (np.exp(x) + 1)\n",
      "C:\\Users\\andra\\AppData\\Local\\Temp\\ipykernel_6776\\3473699835.py:2: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.exp(x) / (np.exp(x) + 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 5)\n",
      "(10, 5)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10000,5) (10000,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_layer\u001b[38;5;241m.\u001b[39mW\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# backwards\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[43moutput_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[72], line 16\u001b[0m, in \u001b[0;36mLayer.adjust_weights\u001b[1;34m(self, alpha, loss_function, preds)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madjust_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, alpha, loss_function,preds):\n\u001b[1;32m---> 16\u001b[0m     P \u001b[38;5;241m=\u001b[39m \u001b[43mderivative_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mderivative_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_function\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZ\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprevious\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma\u001b[49m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m-\u001b[39m alpha \u001b[38;5;241m*\u001b[39m P\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10000,5) (10000,10) "
     ]
    }
   ],
   "source": [
    "class Layer:\n",
    "    def __init__(self,activation_function, previous_layer, size, X):\n",
    "        self.activation_function = activation_function\n",
    "        self.W = np.random.randn(X.shape[1], size)\n",
    "        self.b = np.random.randn(X.shape[0], size)\n",
    "\n",
    "        self.previous = previous_layer\n",
    "        self.X = X\n",
    "\n",
    "    def activate(self):\n",
    "        self.Z = self.X.dot(self.W) + self.b\n",
    "        self.a = self.activation_function(self.Z)\n",
    "        return self.a\n",
    "\n",
    "    def adjust_weights(self, alpha, loss_function,preds):\n",
    "        P = derivative_dict[loss_function](preds,self.a) * derivative_dict[self.activation_function](self.Z) * self.previous.a\n",
    "        self.W = self.W - alpha * P\n",
    "        #self.W = self.W - alpha * derivative_dict[loss_function](self.next.a) * derivative_dict[self.next.activation_function](self.next.z)*self.a\n",
    "\n",
    "# forward\n",
    "l1 = Layer(relu,None,10,train_test)\n",
    "X2 = l1.activate()\n",
    "output_layer = Layer(sigmoid,l1,5,X2)\n",
    "output_layer.activate()\n",
    "preds = output_layer.a\n",
    "#print(preds)\n",
    "print(preds.shape)\n",
    "print(output_layer.W.shape)\n",
    "\n",
    "# backwards\n",
    "output_layer.adjust_weights(0.01,cross_entropy,preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
