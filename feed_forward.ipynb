{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_rows(arr):\n",
    "    row_means = arr.mean(axis=1, keepdims=True)  # Compute the mean of each row\n",
    "    row_stds = arr.std(axis=1, keepdims=True)    # Compute the std of each row\n",
    "\n",
    "    row_stds[row_stds == 0] = 1\n",
    "\n",
    "    standardized_arr = (arr - row_means) / row_stds  # Standardize each row\n",
    "    return standardized_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "train = np.load('./data/fashion_train.npy')\n",
    "\n",
    "\n",
    "\n",
    "train_test = standardize_rows(train[:, : -1])\n",
    "y = train[:,-1]\n",
    "\n",
    "y_matrix = np.eye(5)[y]\n",
    "\n",
    "print(train_test.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def input_layer(X_train, y_train):\n",
    "    n_feartures = X_train.shape[1]\n",
    "    output_nodes = len(set(y_train))\n",
    "    len_of_X_train = len(X_train)\n",
    "    return n_feartures, output_nodes, len_of_X_train\n",
    "\n",
    "\n",
    "def hidden_layer(X, n_feartures, nunbers_of_nodes_hidden, len_of_X_train):\n",
    "    #first hidden layer\n",
    "\n",
    "    W = np.random.randn(n_feartures, nunbers_of_nodes_hidden)\n",
    "\n",
    "    b = np.random.randn(len_of_X_train, nunbers_of_nodes_hidden)\n",
    "\n",
    "    z = X@W+b\n",
    "\n",
    "    a = np.maximum(0, z)\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "#output layer\n",
    "def output_layer(a, output_nodes, len_of_X_train):\n",
    "\n",
    "    W = np.random.randn(a.shape[1], output_nodes)\n",
    "\n",
    "    b = np.random.randn(len_of_X_train, output_nodes)\n",
    "\n",
    "    z = a@W+b\n",
    "\n",
    "    softmax = np.exp(z - np.max(z))/np.sum(np.exp(z - np.max(z)))\n",
    "    preds = np.argmax(softmax, axis=1)\n",
    "\n",
    "    return print(f'pred: {np.argmax(softmax, axis=1)}, shape: {softmax.shape}'), preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: [0 0 0 ... 0 0 0], shape: (10000, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, array([0, 0, 0, ..., 0, 0, 0], dtype=int64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_test\n",
    "\n",
    "n_feartures, output_nodes, len_of_X_train = input_layer(X, y)\n",
    "\n",
    "a = hidden_layer(X, n_feartures, nunbers_of_nodes_hidden=40, len_of_X_train=len_of_X_train)\n",
    "\n",
    "preds = output_layer(a, output_nodes, len_of_X_train)\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return np.exp(x) / (np.exp(x) + 1)\n",
    "\n",
    "def derivative_sigmoid(x):\n",
    "    sig = sigmoid(x)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "def softmax(z):\n",
    "    return np.exp(z - np.max(z))/np.sum(np.exp(z - np.max(z)))\n",
    "\n",
    "def softmax_derivative(z):\n",
    "    s = np.exp(z) / np.sum(np.exp(z))\n",
    "    s = s.reshape(-1, 1)\n",
    "    jacobian = np.diagflat(s) - np.dot(s, s.T)\n",
    "    return jacobian\n",
    "\n",
    "def relu(x, alpha=0.01):\n",
    "    return np.maximum(alpha * x, x)\n",
    "\n",
    "def derivative_relu(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def cross_entropy(y, a, epsilon=1e-12):\n",
    "    a = np.clip(a, epsilon, 1. - epsilon)\n",
    "    return -np.sum(y * np.log(a))\n",
    "\n",
    "def cross_entropy_derivative_a(y, a):\n",
    "    return -1/a * np.sum(y)\n",
    "\n",
    "def cross_entropy_derivative_softmax(y, a):\n",
    "    return a-y\n",
    "\n",
    "def binary_cross_entropy(y, a, epsilon=1e-12):\n",
    "    a = np.clip(a, epsilon, 1. - epsilon)  # Avoid log(0) errors\n",
    "    return -np.mean(y * np.log(a) + (1 - y) * np.log(1 - a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative_dict = {\n",
    "    softmax:softmax_derivative,\n",
    "    cross_entropy:cross_entropy_derivative_a,\n",
    "    sigmoid:derivative_sigmoid,\n",
    "    relu:derivative_relu\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2328"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Layer:\n",
    "    def __init__(self,activation_function,shape):\n",
    "        self.activation_function = activation_function\n",
    "        self.W = np.random.randn(shape[0], shape[1]) * np.sqrt(2 / shape[0])\n",
    "        #self.b = np.random((1, shape[1]))\n",
    "        self.b = np.zeros((1, shape[1]))\n",
    "\n",
    "    def feed(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    def activate(self):\n",
    "        #self.Z = self.X.dot(self.W) + self.b\n",
    "        self.Z = self.X.dot(self.W) + self.b\n",
    "        self.Z = (self.Z - np.mean(self.Z, axis=0)) / np.std(self.Z, axis=0)\n",
    "        self.a = self.activation_function(self.Z)\n",
    "        return self.a\n",
    "\n",
    "    def adjust_weights_output(self, alpha, y_matrix):\n",
    "        dL_dZ = self.a - y_matrix  # Direct gradient for cross-entropy with softmax\n",
    "        dL_w = np.dot(dL_dZ.T, self.X).T\n",
    "\n",
    "        self.W = self.W - alpha * dL_w\n",
    "        self.b = self.b - alpha * dL_dZ.mean(axis=0, keepdims=True)\n",
    "        self.dL_dZ = dL_dZ # error signal  ??? gotta research a bit\n",
    "    \n",
    "    def adjust_weights(self,alpha,next_layer):\n",
    "        da_dZ = derivative_dict[self.activation_function](self.Z)\n",
    "        dL_dZ = np.dot(next_layer.dL_dZ, next_layer.W.T) * da_dZ\n",
    "        dL_w = np.dot(dL_dZ.T, self.X).T\n",
    "\n",
    "        self.W = self.W - alpha * dL_w\n",
    "        self.b = self.b - alpha * dL_dZ.mean(axis=0, keepdims=True)\n",
    "        self.dL_dZ = dL_dZ\n",
    "\n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(self,X,layer_info):\n",
    "        self.X = X\n",
    "        self.layer_nums = layer_info\n",
    "        self.layers = []\n",
    "\n",
    "        for i in range(len(layer_info)-1):\n",
    "            l = Layer(layer_info[i][\"activation_function\"],(layer_info[i][\"size\"],layer_info[i+1][\"size\"]))\n",
    "            self.layers.append(l)\n",
    "    \n",
    "\n",
    "\n",
    "#forward\n",
    "input_layer = Layer(relu,(784,20))\n",
    "h1 = Layer(relu,(20,10))\n",
    "output_layer = Layer(softmax,(10,5))\n",
    "input_layer.feed(train_test)\n",
    "\n",
    "for i in range(1000):\n",
    "    input_layer.activate()\n",
    "    h1.feed(input_layer.a)\n",
    "    h1.activate()\n",
    "    output_layer.feed(h1.a)\n",
    "    output_layer.activate()\n",
    "\n",
    "    output_layer.adjust_weights_output(0.0001,y_matrix)\n",
    "    h1.adjust_weights(0.0001,output_layer)\n",
    "    input_layer.adjust_weights(0.0001,h1)\n",
    "\n",
    "preds=output_layer.a.argmax(axis=1)\n",
    "sklearn.metrics.accuracy_score(preds,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2496"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds=output_layer.a.argmax(axis=1)\n",
    "sklearn.metrics.accuracy_score(preds,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.97709696e-01, 1.34624998e-06, 9.99999997e-01, 8.87216426e-01,\n",
       "        1.87055661e-02],\n",
       "       [9.99999318e-01, 2.14961487e-07, 9.99999866e-01, 6.53627756e-01,\n",
       "        1.07191168e-01],\n",
       "       [7.37926362e-01, 1.72396161e-07, 1.00000000e+00, 9.14274671e-01,\n",
       "        1.04642128e-02],\n",
       "       ...,\n",
       "       [1.00000000e+00, 2.89932063e-13, 1.00000000e+00, 8.41873535e-01,\n",
       "        9.45439227e-03],\n",
       "       [8.93434140e-01, 1.55613148e-07, 1.00000000e+00, 9.04027772e-01,\n",
       "        1.23144157e-02],\n",
       "       [9.99999927e-01, 3.56340735e-15, 1.00000000e+00, 9.61700721e-01,\n",
       "        7.72121239e-04]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2076"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
